{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ngram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgN97_pM7FP6",
        "colab_type": "text"
      },
      "source": [
        "referensi: https://yasirutomo.com/blog/7/spell-checker-untuk-deteksi-dan-perbaikan-typo-bahasa-indonesia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HibPnoGqtCwP",
        "colab_type": "code",
        "outputId": "3de82a80-584d-40ba-e762-b981c623bb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install Sastrawi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 1.4MB/s \n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKmTi0UJ7Y5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from collections import Counter\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "firc4-Nx8Vx2",
        "colab_type": "code",
        "outputId": "032b6376-d815-48cb-9c9b-7d92bef4d1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6886Q-3C79D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pjsMYdW8BJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kalimat = \"Aku sedang mengerjakan tugas kuliah di rumah\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA6mHhwK8MCy",
        "colab_type": "code",
        "outputId": "dd69e90f-3a07-46aa-fec6-7da6823f48a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"1-gram: \", extract_ngrams(kalimat, 1))\n",
        "print(\"2-gram: \", extract_ngrams(kalimat, 2))\n",
        "print(\"3-gram: \", extract_ngrams(kalimat, 3))\n",
        "print(\"4-gram: \", extract_ngrams(kalimat, 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram:  ['Aku', 'sedang', 'mengerjakan', 'tugas', 'kuliah', 'di', 'rumah']\n",
            "2-gram:  ['Aku sedang', 'sedang mengerjakan', 'mengerjakan tugas', 'tugas kuliah', 'kuliah di', 'di rumah']\n",
            "3-gram:  ['Aku sedang mengerjakan', 'sedang mengerjakan tugas', 'mengerjakan tugas kuliah', 'tugas kuliah di', 'kuliah di rumah']\n",
            "4-gram:  ['Aku sedang mengerjakan tugas', 'sedang mengerjakan tugas kuliah', 'mengerjakan tugas kuliah di', 'tugas kuliah di rumah']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTaoijl_AbL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hurufKecil = kalimat.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TEH7Pm2AgoW",
        "colab_type": "code",
        "outputId": "a50ddf84-a246-42e0-84b8-f1e00b752df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hurufKecil"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aku sedang mengerjakan tugas kuliah di rumah'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY5t8NvBE7ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words(text): return re.findall(r'w+', text.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFYaa8zoIggE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/kata_dasar (1).txt', 'r') as file:\n",
        "   WORDS = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WOnoIfyIiNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter(WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRa-tjdCNNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = open('/content/drive/My Drive/kata_dasar (3).txt').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_u9mENyCT96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter(WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxpPnsivUHm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zce6YAa_CKNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def P(word, N=sum(WORDS.values())):\n",
        "    # \"Probability of `word`.\"\n",
        "    #print(WORDS[word] / N)\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    # \"Most probable spelling correction for word.\"\n",
        "    # print(max(candidates(word), key=P))\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    # \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    # \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    # \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)] # [('', 'kemarin'), ('k', 'emarin'), ('ke', 'marin'), dst]\n",
        "    # print(splits)\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R] # ['emarin', 'kmarin', 'kearin', dst]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] # ['ekmarin', 'kmearin', 'keamrin', dst]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # ['aemarin', 'bemarin', 'cemarin', dst]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters] # ['akemarin', 'bkemarin', 'ckemarin', dst]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    # \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzazGwFGMNWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kata = 'minummm'\n",
        "print('kata typo : ', kata)\n",
        "print('koreksi : ', stemmer.stem(correction(kata)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}